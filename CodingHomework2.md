# CodingHomework2

## 林仲航	2020210863



## 1. 算法

采用随机森林算法进行分类训练，在随机森林的每个节点，随机选取$M$个特征进行计算，其中每棵树均为CartTree，为了减小树的复杂程度，加入了最大树深以及每个节点最少样本数两个超参数，当树深度达到最大树深或样本数小等于最少样本数时，强行停止计算并将此节点当作叶子节点。

## 2. 数据预处理

由于采用随机森林算法，故而除了缺失值外，并不需要进行额外的数据预处理工作，但是为了算法内部的高效，将所有离散值变量数据映射为从0开始的整数值（numpy中对数值型array的索引速度快于原始的字符串数组）。此外，由于原数据中描述特征duration与样本的标签y高度相关，故而在此次实验中删去该特征信息。

对于缺失值的处理，直接在数据送入随机森林前完成，随机森林本身不负责处理缺失值，通过对数据缺失值情况的观察，["default", "pays"]两个字段缺失值较多，故而对该数据缺失值填补总体策略如下：

- 对于非default字段的离散数据，利用样本中的众数进行填充
- 对于default字段数据，直接将缺失值当作特征的一个值来看待
- 对于连续性变量，采用均值来替代缺失值

该部分代码详见data_preprocess.py。



## 3. 正负样本不均衡

由于随机森林本身为集成学习算法，可以用于处理正负样本不均衡问题。通过观察，该数据集中正样本树远小于负样本数。故而在随机森林的自举采样中，我们可以对自举采样进行变形，利用处理不均衡问题中常用的下采样方法。具体做法为：

1. 在$N$个正样本中，利用自举采样得到包含$N$个正样本的集合$X_1$
2. 在负样本中自举采样得到包含$N$个负样本的集合$X_0$
3. 两个集合合并后送入CartTree中进行训练

